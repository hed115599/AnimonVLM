{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6d98c-0728-4fb1-b872-6451717e1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Assume utils is in current directory\n",
    "sys.path.append('./') \n",
    "from utils.data_utils import vis_FLlabels\n",
    "\n",
    "# ========== 1. Load Model ==========\n",
    "def load_model(base_model_path: str, peft_model_path: str, device: torch.device):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f\"üöÄ Loading base model...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        attn_implementation=\"eager\" \n",
    "    )\n",
    "    print(\"üîÑ Loading LoRA weights...\")\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "    processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "# ========== 2. Inference Function ==========\n",
    "def infer_single_image(model, processor, image_path, task, text_input=None, bbox_str=None, device=None):\n",
    "    \"\"\"\n",
    "    General inference function\n",
    "    - text_input: For Grounding tasks, e.g., \"pig\"\n",
    "    - bbox_str: For tasks requiring region input like REGION_TO_SEGMENTATION\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # --- Construct Prompt ---\n",
    "    if bbox_str:\n",
    "        # Scenario A: Region task (Task + BBox)\n",
    "        loc_start = bbox_str.find(\"<\")\n",
    "        clean_bbox = bbox_str[loc_start:] if loc_start != -1 else bbox_str\n",
    "        question = task + clean_bbox\n",
    "    elif text_input:\n",
    "        # Scenario B: Text grounding task (Task + Text)\n",
    "        question = task + text_input\n",
    "    else:\n",
    "        # Scenario C: Pure task\n",
    "        question = task\n",
    "    \n",
    "    print(f\"  üìù Prompt: {question[:100]}...\")  # Print first 100 characters\n",
    "        \n",
    "    inputs = processor(text=question, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                pixel_values=inputs[\"pixel_values\"],\n",
    "                max_new_tokens=1024,\n",
    "                num_beams=3,\n",
    "                do_sample=False\n",
    "            )\n",
    "    \n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    return generated_text.replace('<s>', '').replace('</s>', '')\n",
    "\n",
    "# ========== 3. Main Program ==========\n",
    "def main():\n",
    "    # --- Configuration ---\n",
    "    target_device = 'cuda:1'\n",
    "    base_path = '/sdb1_hdisk/pub_data/MODELS/Florence-2-large-ft/'\n",
    "    peft_path = '/sdb1_hdisk/pub_data/chenhong/Florence2/results/KEYPOINT_ONLY/epoch_299/'\n",
    "    img_path = '/sdb1_hdisk/pub_data/DATAS/BamaPig2D/images/000000.png'\n",
    "    \n",
    "    try:\n",
    "        device = torch.device(target_device)\n",
    "        torch.cuda.get_device_name(device)\n",
    "    except:\n",
    "        device = torch.device('cuda:0')\n",
    "    print(f\"üéØ Using device: {device}\")\n",
    "\n",
    "    # Load model\n",
    "    model, processor = load_model(base_path, peft_path, device)\n",
    "\n",
    "    # ==========================================\n",
    "    # ‚ö†Ô∏è Modify task type here\n",
    "    # ==========================================\n",
    "    current_task = \"<KEYPOINT>\"\n",
    "    \n",
    "    # =====================================================\n",
    "    # üîÄ Task Branch Logic\n",
    "    # =====================================================\n",
    "    \n",
    "    # --- Branch 1: Tasks requiring detection boxes first (e.g., SEGMENTATION) ---\n",
    "    if current_task == \"<REGION_TO_SEGMENTATION>\":\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç [Step 1] Using base model to detect all 'pig' bounding boxes...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # üî• Disable LoRA, use base model for detection\n",
    "        with model.disable_adapter():\n",
    "            od_result = infer_single_image(\n",
    "                model, processor, img_path, \n",
    "                task=\"<CAPTION_TO_PHRASE_GROUNDING>\", \n",
    "                text_input=\"pig\", \n",
    "                device=device\n",
    "            )\n",
    "            print(f\"üì¶ Detection result: {od_result}\")\n",
    "            \n",
    "            # Extract all bounding boxes\n",
    "            pattern = r'(<loc_\\d+>){4}'\n",
    "            all_boxes = [m.group(0) for m in re.finditer(pattern, od_result)]\n",
    "        \n",
    "        print(f\"‚úÖ Detected {len(all_boxes)} pigs\")\n",
    "        \n",
    "        if len(all_boxes) == 0:\n",
    "            print(\"‚ùå No pigs detected, exiting.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç [Step 2] Using LoRA model to segment each pig...\")\n",
    "        print(\"=\"*60)\n",
    "        # LoRA automatically restored\n",
    "        \n",
    "        final_seg_str = \"\"\n",
    "        \n",
    "        for i, box in enumerate(all_boxes):\n",
    "            print(f\"\\n  üëâ [{i+1}/{len(all_boxes)}] Processing box: {box}\")\n",
    "            \n",
    "            # Segment current box\n",
    "            seg_res = infer_single_image(\n",
    "                model, processor, img_path, \n",
    "                task=\"<REGION_TO_SEGMENTATION>\", \n",
    "                bbox_str=box, \n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            print(f\"  ‚úÖ Segmentation result length: {len(seg_res)} characters\")\n",
    "            print(f\"  üìÑ First 100 characters: {seg_res[:100]}...\")\n",
    "            \n",
    "            # Concatenate results (separate different pigs with <sep>)\n",
    "            if i > 0:\n",
    "                final_seg_str += \"<sep>\"\n",
    "            final_seg_str += seg_res\n",
    "        \n",
    "        print(f\"\\n‚úÖ All segmentation completed! Total length: {len(final_seg_str)} characters\")\n",
    "        \n",
    "        # Visualization parameters\n",
    "        kwargs = {\n",
    "            'img': img_path,\n",
    "            'resize': 0.5,           # Resize for easier viewing\n",
    "            'show': False,\n",
    "            'FLlabel': final_seg_str  # ‚ö†Ô∏è Note: Segmentation results use FLlabel parameter\n",
    "        }\n",
    "    \n",
    "    # --- Branch 2: Direct tasks (e.g., OD, KEYPOINT, etc.) ---\n",
    "    elif current_task == \"<OD>\":\n",
    "        print(f\"\\nüîç Executing task: {current_task}\")\n",
    "        res = infer_single_image(model, processor, img_path, current_task, device=device)\n",
    "        print(f\"\\n‚ú® Raw output: {res}\")\n",
    "        \n",
    "        kwargs = {\n",
    "            'img': img_path,\n",
    "            'resize': 0.5,\n",
    "            'show': False,\n",
    "            'FLbbox': res\n",
    "        }\n",
    "    \n",
    "    elif current_task == \"<POINT>\":\n",
    "        print(f\"\\nüîç Executing task: {current_task}\")\n",
    "        res = infer_single_image(model, processor, img_path, current_task, device=device)\n",
    "        print(f\"\\n‚ú® Raw output: {res}\")\n",
    "        \n",
    "        kwargs = {\n",
    "            'img': img_path,\n",
    "            'resize': 0.5,\n",
    "            'show': False,\n",
    "            'FLpoint': res\n",
    "        }\n",
    "    \n",
    "    elif current_task == \"<KEYPOINT>\":\n",
    "        print(f\"\\nüîç Executing task: {current_task}\")\n",
    "        \n",
    "        # Keypoint task also requires two steps\n",
    "        print(\"\\nüîç [Step 1] Detecting all pigs...\")\n",
    "        with model.disable_adapter():\n",
    "            od_result = infer_single_image(\n",
    "                model, processor, img_path, \n",
    "                task=\"<CAPTION_TO_PHRASE_GROUNDING>\", \n",
    "                text_input=\"pig\", \n",
    "                device=device\n",
    "            )\n",
    "            pattern = r'(<loc_\\d+>){4}'\n",
    "            all_boxes = [m.group(0) for m in re.finditer(pattern, od_result)]\n",
    "        \n",
    "        print(f\"‚úÖ Detected {len(all_boxes)} pigs\")\n",
    "        \n",
    "        if len(all_boxes) == 0:\n",
    "            print(\"‚ùå No pigs detected\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüîç [Step 2] Detecting keypoints...\")\n",
    "        final_kp_str = \"\"\n",
    "        \n",
    "        for i, box in enumerate(all_boxes):\n",
    "            print(f\"  üëâ [{i+1}/{len(all_boxes)}] Processing box: {box}\")\n",
    "            kp_res = infer_single_image(\n",
    "                model, processor, img_path, \n",
    "                task=\"<KEYPOINT>\", \n",
    "                bbox_str=box, \n",
    "                device=device\n",
    "            )\n",
    "            final_kp_str += kp_res\n",
    "        \n",
    "        kwargs = {\n",
    "            'img': img_path,\n",
    "            'resize': 0.5,\n",
    "            'show': False,\n",
    "            'FLkeypoint': final_kp_str\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(f\"‚ùå Unsupported task type: {current_task}\")\n",
    "        return\n",
    "    \n",
    "    # =====================================================\n",
    "    # üé® Unified Visualization\n",
    "    # =====================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé® Calling vis_FLlabels for visualization...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        result_bgr = vis_FLlabels(**kwargs)\n",
    "        \n",
    "        if result_bgr is not None:\n",
    "            result_rgb = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)\n",
    "            result_pil = Image.fromarray(result_rgb)\n",
    "            display(result_pil)\n",
    "            \n",
    "            # Save result\n",
    "            save_name = f\"result_{current_task.strip('<>').lower()}.png\"\n",
    "            result_pil.save(save_name)\n",
    "            print(f\"üíæ Result saved: {save_name}\")\n",
    "        else:\n",
    "            print(\"‚ùå Visualization result is empty (None)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animonvlm",
   "language": "python",
   "name": "animonvlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
